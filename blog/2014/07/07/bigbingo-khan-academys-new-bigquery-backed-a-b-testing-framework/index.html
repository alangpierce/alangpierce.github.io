
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>BigBingo: Khan Academy's New BigQuery-backed A/B Testing Framework - Alan Pierce</title>
  <meta name="author" content="Alan Pierce">

  
  <meta name="description" content="In late January, I joined Khan Academy as the third member of the
infrastructure team. We were just starting a big performance push, so I spent my &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.alangpierce.com/blog/2014/07/07/bigbingo-khan-academys-new-bigquery-backed-a-b-testing-framework">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Alan Pierce" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-52164083-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Alan Pierce</a></h1>
  
    <h2>Software Engineer at Khan Academy</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:www.alangpierce.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/about">About</a></li>
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">BigBingo: Khan Academy's New BigQuery-backed A/B Testing Framework</h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-07T12:00:00-07:00" pubdate data-updated="true">Jul 7<sup>th</sup>, 2014</time>
        
           | <a href="#disqus_thread"
             data-disqus-identifier="http://www.alangpierce.com">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>In late January, I joined Khan Academy as the third member of the
infrastructure team. We were just starting a big performance push, so I spent my
first week or two improving our
<a href="https://github.com/kamens/gae_mini_profiler">profiling tools</a> and finding and fixing some easy
slow spots (including speeding up the home page by over a second). However, every time I profiled
any page, I found that the A/B testing framework,
<a href="https://github.com/kamens/gae_bingo">GAE/Bingo</a>, was always one of the
slowest pieces. I had some ideas on how to make some incremental improvements to speed
it up, but instead, I was given a much more ambitious project: to rethink
and rewrite the whole A/B testing system from scratch. I had plenty of
sources of guidance and advice, but I, the new guy, was to be the sole owner and author of the
new system. I was up for the task, but it was nevertheless a bit daunting.</p>

<p>Instead of the old strategy of keeping the A/B test data continuously
up-to-date using memcache (and periodically flushing to the App Engine
datastore), the new system would report events by simply logging
them, and those log statements would eventually make their way into
<a href="https://developers.google.com/bigquery/">Google BigQuery</a> through an hourly
<a href="https://github.com/Khan/appengine-mapreduce">MapReduce</a> job based on
<a href="https://code.google.com/p/log2bq/">log2bq</a>. From there, the real A/B test
processing would be done completely in SQL using BigQuery queries. Since we
were revamping GAE/Bingo using BigQuery, there was an obvious name: BigBingo.</p>

<p>Of course, that three-sentence description leaves out pretty much all of the
details and makes some dangerous assumptions, but the high-level plan ended up
working (with some tweaks), and I&rsquo;m happy to say that all A/B tests at Khan Academy are now
running under BigBingo, and the last remnants of the old GAE/Bingo system are finally
being removed. In this post, I&rsquo;ll talk about why a rewrite was so important, how
we think about A/B testing, and some specific points of the design and
architecture of BigBingo. There are some additional cool details that are probably
deserving of their own blog post, so look out for those in the future.</p>

<h2>BigBingo is fast!</h2>

<p>Most developers at Khan Academy had a sense that the old GAE/Bingo system was slow and BigBingo
would improve overall performance, but I doubt anybody expected that the
improvement would be as dramatic as it was. When I finally flipped the switch to
turn off GAE/Bingo, the average latency across <em>all</em> requests went from a little
over 300ms to a little under 200ms. The most important pages had even better
results, but I&rsquo;ll let the pictures do the talking:</p>

<p>The logged-in homepage got twice as fast:</p>

<p><img src="/images/logged_in_homepage.png" alt="Logged-in homepage percentiles" /></p>

<p>The logged-out homepage improved even more:</p>

<p><img src="/images/logged_out_homepage.png" alt="Logged-out homepage percentiles" /></p>

<p>And our memcache went from &ldquo;worryingly overloaded&rdquo; to &ldquo;doing great&rdquo;:</p>

<p><img src="/images/memcache_usage.png" alt="Memcache compute units" /></p>

<p>Of course, making the site faster makes users happier, but it has another big
benefit: cost savings. If requests can be processed twice as fast, we only need
half as many App Engine instances running at a given time, so our App Engine
bill drops significantly. Since Khan Academy is a nonprofit running off of
donations, it&rsquo;s important to us to have an efficient infrastructure so we can
focus our money on improving education, not upkeep.</p>

<h2>A/B testing at Khan Academy</h2>

<p>A/B testing isn&rsquo;t just some occasional tool at Khan Academy; it&rsquo;s an important
part of our engineering culture, and almost any change that we care about goes
through an A/B test first, often multiple A/B tests. Right now, there are 57
A/B tests actively running, which is an average of about two active A/B tests
per developer.</p>

<p>Unlike &ldquo;traditional&rdquo; A/B testing (which tends to maximize simple metrics like
ad clicks, purchases, etc.), Khan Academy&rsquo;s A/B testing tries to maximize
student <em>learning</em>. That means that we try out much more advanced changes than
just little UI tweaks, and measuring success is a huge challenge by itself.
Here are some examples of A/B tests we do:</p>

<ul>
<li>We have a <a href="http://mattfaus.com/2014/07/khan-academy-mastery-mechanics/">sophisticated system</a>
that tries to understand a student&rsquo;s knowledge level and recommend the best
exercises for them, and we&rsquo;re always making little improvements to it. For
example, we recently tried out a new system to detect when users are below
their learning edge and advance them through the material more quickly.
Learners under the new system progressed further, as expected, and they almost
always stayed at their advanced level rather than being demoted, so we rolled
out the new algorithm to all users.</li>
<li>We&rsquo;ve been experimenting with providing message snippets to teach our users
that learning makes them not just more knowledgeable, but smarter
as well. This specific motivational approach turns out to be surprisingly
effective, and results in increased site usage and learning outcomes, so we&rsquo;re
trying out various different approaches to deliver the message in the most
effective way.</li>
<li>We recently switched the homepage background to one we liked better. It didn&rsquo;t improve any
metrics noticeably, but the A/B test verified that it didn&rsquo;t hurt anything
either, so we kept the new background. We run lots of little experiments like this one.</li>
</ul>


<h2>What&rsquo;s different about BigBingo?</h2>

<p>In the years since GAE/Bingo was written, the devs at KA learned
<a href="http://bjk5.com/post/28269263789/lessons-learned-a-b-testing-with-gae-bingo">a thing or two</a>
about the right way to do A/B testing and what an A/B testing framework should really do, so
BigBingo diverges from GAE/Bingo in a few important ways.</p>

<h3>The data</h3>

<p>Here&rsquo;s what you&rsquo;d see when looking at the latest results of an old GAE/Bingo
experiment (I added a red box to indicate the &ldquo;real&rdquo; data; everything else is derived
from those numbers):</p>

<p><img src="/images/gae_bingo.png" alt="GAE/Bingo Dashboard" /></p>

<p>For clear-cut results, a few numbers will do just fine, but what do you do when
the results are unexpected or completely nonsensical? In GAE/Bingo, the best
thing you could do was shrug and speculate about what happened. BigBingo is
different: we keep around all raw results (user-specific conversion totals) as well as
the source logs and the intermediate data used to determine those results. Since
it&rsquo;s all in BigQuery, investigating anomalies is just a matter of doing some
digging using SQL.</p>

<p>Keeping the raw data also makes it easy to do more advanced analysis
after-the-fact:</p>

<ul>
<li>Instead of just using the mean number of conversions, you can look at more
interesting statistics like the median, percentiles, and standard deviation,
and you can ignore outliers.</li>
<li>You can cross-reference A/B test participation with more sophisticated
metrics, like the
<a href="https://sites.google.com/a/khanacademy.org/forge/khan-academy-data-science-public-documentation/learning-gain">learning gain</a>
metric that the data science team is working on.</li>
<li>You can segment your analysis based on any property you can come up with. For
example, you might want to focus on only new users or only long-term users.</li>
</ul>


<h3>Some other differences</h3>

<ul>
<li>Instead of experiments needing to pick their metrics up-front, every
experiment automatically tracks <em>every</em> conversion (currently we have about 200
of them).</li>
<li>Since KA already has a culture of A/B testing, BigBingo encourages
high-quality experiments rather than focusing on making experiments as easy as
possible. Every experiment has an owner assigned and a description explaining
what the experiment is for and the experimental hypothesis. When an A/B test is
stopped, the author is forced to fill in a conclusion. Whenever an experiment
starts or finishes, a notification is sent to the entire team, so it&rsquo;s easy to see what
kinds of ideas everyone else is trying out and how they are going.</li>
<li>BigBingo doesn&rsquo;t try to be real-time, which makes the implementation much
simpler. After all, up-to-the-minute A/B test results are
<a href="http://bjk5.com/post/12829339471/a-b-testing-still-works-sarcastic-phew">pretty useless</a>
anyway.</li>
<li>The use of memcache counters added
<a href="http://bjk5.com/post/36567537399/dangers-of-using-memcache-counters-for-a-b-tests">a little bit of complexity</a>
to GAE/Bingo, which I was happy to get rid of. Not only were there complex
details, running BigBingo and GAE/Bingo side-by-side revealed some additional
race conditions in GAE/Bingo that weren&rsquo;t known yet.</li>
</ul>


<h2>Implementation</h2>

<p>Here&rsquo;s a big-picture overview of what BigBingo looks like:</p>

<p><img src="/images/architecture.png" alt="The architecture" /></p>

<p>Here&rsquo;s how the data flows from a user-facing request to BigQuery, then to the
dashboard UI:</p>

<ol>
<li>When a user enters into an A/B test, that event is recorded through a log
statement. The user&rsquo;s alternative is chosen through a deterministic function
similar to <code>hash(experiment_name + user_id) % num_alternatives</code>, so no RPCs are
necessary to coordinate that information.</li>
<li>When a user triggers a conversion event, it is recorded through a log
statement.</li>
<li>In the hourly LogToBigQuery log export process, the raw log events (called
&ldquo;bingo events&rdquo;) are parsed and extracted into custom BigQuery columns to be
included in the normal request logs tables.</li>
<li>Every two hours, the BigBingo Summarize task runs and processes the new logs
to compute the latest A/B test numbers, following a few rules:

<ul>
<li>If a user participates multiple times in an A/B test (which is common),
only the earliest event counts.</li>
<li>A conversion event only counts for an experiment if the event happened
after the user first participated in the experiment.</li>
<li>For each conversion, BigBingo computes both the total number of times the
conversion was triggered and the total number of distinct users that
triggered the conversion.</li>
</ul>
</li>
<li>The latest data is cleaned up and copied to a &ldquo;publish&rdquo; dataset where it can
be conveniently accessed.</li>
<li>The BigBingo dashboard, a web UI, queries these results to disply all data
about a given experiment: the historical participant and conversion numbers, as
well as p-values for each alternative.</li>
</ol>


<p>Most of the details are reasonably straightforward, but I&rsquo;ll dig into what&rsquo;s
probably the most controversial aspect of this architecture: the decision to
use Google BigQuery for all storage and processing.</p>

<h3>About BigQuery</h3>

<p>If you&rsquo;re not familiar with BigQuery, it&rsquo;s a hosted Google service (really an
externalization of an internal Google service called
<a href="http://research.google.com/pubs/pub36632.html">Dremel</a>) that allows
you to import giant datasets and run nearly-arbitrary SQL queries on them.
BigQuery is way faster than MapReduce-based SQL engines like Hive: you&rsquo;re
borrowing thousands of machines from Google for just the duration of your
query, and all work is done in-memory, so queries tend to finish in just a few
seconds. The primary use case for BigQuery is for human users to manually dig
into data, but I&rsquo;ll show how it can also be used to build stateful data
pipelines.</p>

<p>BigQuery supports nearly all SQL, but don&rsquo;t let that fool you into thinking it&rsquo;s
anything close to a relational database! It has a small set of primitives that&rsquo;s
different from anything I&rsquo;ve worked with before:</p>

<table>
<thead>
<tr>
<th></th>
<th> Operation </th>
<th> Price </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Import CSV/JSON data into a table </td>
<td> Free</td>
</tr>
<tr>
<td></td>
<td> Run a SELECT query </td>
<td> 0.5 cents per GB in all columns touched</td>
</tr>
<tr>
<td></td>
<td> Store a query result as a new table </td>
<td> Free</td>
</tr>
<tr>
<td></td>
<td> Apppend query results to the end of a table </td>
<td> Free</td>
</tr>
<tr>
<td></td>
<td> Copy a table </td>
<td> Free</td>
</tr>
</tbody>
</table>


<br />


<p>There are
<a href="https://developers.google.com/bigquery/docs/reference/v2/">a few more operations</a>
that are less common, but the ones I listed are the most common ones.</p>

<p>Notice anything missing? No transactions? Not even a way to update or delete
rows? No way to pull out a single row without paying for the whole table? How
can you possibly keep track of A/B test results in such a restricted system?
You&rsquo;re pretty much stuck with the following rule:</p>

<p><strong>To update a table, you must completely rebuild it from scratch with the new
values.</strong></p>

<h3>That&rsquo;s crazy, right?</h3>

<p>It certainly feels like an architectural sin to rebuild all of your data over
and over, but it&rsquo;s not as unreasonable as you might think. BigQuery is quite
cost-efficient (some rough numbers suggest that it&rsquo;s more than 10x as
cost-efficient as MapReduce running on App Engine), and there are lots of
little tricks you can do to reduce the size of your tables. By designing the
table schemas with space-efficiency in mind, I was able to reduce BigBingo&rsquo;s
data usage from 1TB ($5 per query) to 50GB (25 cents per query). (I&rsquo;ll go over
the details in a future blog post.)</p>

<p>There are also some huge usability advantages to using BigQuery over
another batch processing system like MapReduce:</p>

<ul>
<li>When I was designing the queries and schema, I could try things out on real
production data from within the BigQuery web UI and get results back in
seconds. This meant that I could work through almost all architectural details
before having to write a line of Python code.</li>
<li>Once I did start to write code, I could run the full job completely from my
laptop, with no need to push code out to servers in order to iterate. Whenever
a query had a problem, it showed up in the &ldquo;Query History&rdquo; section of the
BigQuery web UI, and I could easily debug it there.</li>
<li>Sanity-checking the intermediate steps and hunting down problems in the data
was easy because everything was immediately accessible through SQL.</li>
</ul>


<h3>Taking advantage of immutable storage</h3>

<p>At first, having to deal with only immutable tables felt like an annoying
restriction that I just had to live with, but as soon as I started thinking
about making the system robust, immutability was a huge benefit. When thinking
through the details, I discovered some important lessons:</p>

<ul>
<li><strong>Never append to the end of a table. Keep tables immutable and queries
idempotent.</strong></li>
<li><strong>A table&rsquo;s name should exactly define its contents.</strong></li>
</ul>


<p>This is probably best explained by looking at a simple data pipeline similar to
BigBingo. First, I&rsquo;ll give a straightforward but fragile approach, then show
how it can be improved to take advantage of BigQuery&rsquo;s architecture.</p>

<p><strong>Goal:</strong> Keep track of the median number of problems solved, problems
attempted, and hints taken across all users.</p>

<p>Every hour, the following queries are done to update the <code>latest_medians</code>
table:</p>

<p><strong>Step 1:</strong> Extract the events from the logs table into a table called
<code>new_event_totals</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="c1">-- Results are written to new_event_totals.</span>
</span><span class='line'><span class="k">SELECT</span>
</span><span class='line'>    <span class="n">user_id</span><span class="p">,</span>
</span><span class='line'>    <span class="c1">-- Count the number of times the event column matches each event name.</span>
</span><span class='line'>    <span class="k">SUM</span><span class="p">(</span><span class="n">event</span> <span class="o">=</span> <span class="ss">&quot;problem_correct&quot;</span><span class="p">)</span> <span class="k">AS</span> <span class="n">problem_correct_count</span><span class="p">,</span>
</span><span class='line'>    <span class="k">SUM</span><span class="p">(</span><span class="n">event</span> <span class="o">=</span> <span class="ss">&quot;problem_attempt&quot;</span><span class="p">)</span> <span class="k">AS</span> <span class="n">problem_attempt_count</span><span class="p">,</span>
</span><span class='line'>    <span class="k">SUM</span><span class="p">(</span><span class="n">event</span> <span class="o">=</span> <span class="ss">&quot;hint_taken&quot;</span><span class="p">)</span> <span class="k">AS</span> <span class="n">hint_taken_count</span><span class="p">,</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">logs_2014_07_01</span>
</span><span class='line'><span class="k">WHERE</span> <span class="n">time</span> <span class="o">&gt;=</span> <span class="mi">1404194400</span> <span class="k">AND</span> <span class="n">time</span> <span class="o">&lt;</span> <span class="mi">1404198000</span>
</span><span class='line'><span class="k">GROUP</span> <span class="k">EACH</span> <span class="k">BY</span> <span class="n">user_id</span>  <span class="c1">-- GROUP EACH BY is just a large-scale GROUP BY</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>Step 2:</strong> Combine <code>new_event_totals</code> with the previous <code>full_event_totals</code>
table to make the new <code>full_event_totals</code> table:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="c1">-- Results are written to full_event_totals by querying to a temp table and</span>
</span><span class='line'><span class="c1">-- copying over full_event_totals.</span>
</span><span class='line'><span class="k">SELECT</span>
</span><span class='line'>    <span class="n">user_id</span><span class="p">,</span>
</span><span class='line'>    <span class="k">SUM</span><span class="p">(</span><span class="n">problem_correct_count</span><span class="p">)</span> <span class="k">AS</span> <span class="n">problem_correct_count</span><span class="p">,</span>
</span><span class='line'>    <span class="k">SUM</span><span class="p">(</span><span class="n">problem_attempt_count</span><span class="p">)</span> <span class="k">AS</span> <span class="n">problem_attempt_count</span><span class="p">,</span>
</span><span class='line'>    <span class="k">SUM</span><span class="p">(</span><span class="n">hint_taken_count</span><span class="p">)</span> <span class="k">AS</span> <span class="n">hint_taken_count</span><span class="p">,</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">new_event_totals</span><span class="p">,</span> <span class="n">full_event_totals</span> <span class="c1">-- UNION ALL</span>
</span><span class='line'><span class="k">GROUP</span> <span class="k">EACH</span> <span class="k">BY</span> <span class="n">user_id</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>Step 3:</strong> Find the median of each metric, and write the result to a table
called <code>latest_medians</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="c1">-- Results are written to latest_medians.</span>
</span><span class='line'><span class="k">SELECT</span>
</span><span class='line'>    <span class="n">NTH</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">QUANTILES</span><span class="p">(</span><span class="n">problem_correct_count</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span> <span class="k">AS</span> <span class="n">median_problems_correct</span><span class="p">,</span>
</span><span class='line'>    <span class="n">NTH</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">QUANTILES</span><span class="p">(</span><span class="n">problem_attempt_count</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span> <span class="k">AS</span> <span class="n">median_problems_attempted</span><span class="p">,</span>
</span><span class='line'>    <span class="n">NTH</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">QUANTILES</span><span class="p">(</span><span class="n">hint_taken_count</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span> <span class="k">AS</span> <span class="n">median_hints_taken</span><span class="p">,</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">full_event_totals</span>
</span></code></pre></td></tr></table></div></figure>


<p>This code ends up working, but it doesn&rsquo;t handle failure very well:</p>

<ul>
<li>Step 2 isn&rsquo;t idempotent. For certain errors (e.g. a connection timeout when
submitting the query), there&rsquo;s no way to know for sure if it&rsquo;s safe to retry,
or if it succeeded in the first place.</li>
<li>If the job fails between steps 2 and 3, it can&rsquo;t be safely retried, so you
need to either manually re-run step 3 or live with out-of-date results for an
hour.</li>
<li>If the job fails before step 2 finishes and isn&rsquo;t retried before the
next job runs, the event_totals table will lose all events from that hour.</li>
<li>If the logs weren&rsquo;t successfully loaded into BigQuery, Step 1 will think that
nothing happend in that hour and will silently compute the wrong results.</li>
</ul>


<p>To solve all of these problems, just <strong>include a timestamp in each table&rsquo;s
name</strong>. The background job then takes as a parameter the particular hour to
process, rather than trying to figure out what the &ldquo;latest&rdquo; hour is. Here&rsquo;s
what it would do if you run it with the hour from 6:00 to 7:00 on July 1:</p>

<p><strong>Step 1:</strong> Read from <code>logs_2014_07_01_06</code> (the logs for 6:00 to 7:00 on July
1) and write to the table <code>new_event_totals_logs_2014_07_01_06</code> (the new events
for 6:00 to 7:00 on July 1).</p>

<p><strong>Step 2:</strong> Read from <code>new_event_totals_logs_2014_07_01_06</code>
and <code>full_event_totals_2014_07_01_06</code> and write to the table
<code>full_event_totals_2014_07_01_07</code> (the full totals as of 7:00 on July 1).</p>

<p><strong>Step 3:</strong> Read from <code>full_event_totals_2014_07_01_07</code> and write to the table
<code>latest_medians_2014_07_01_07</code> (the medians as of 7:00 on July 1).</p>

<p>The job takes the hour to process as a parameter, and reads the previous hour&rsquo;s
tables to generate that hour&rsquo;s tables. Making three new tables per hour may
seem wasteful, but it&rsquo;s actually <strong>just as easy and cheap as the previous
scheme</strong>. The main problem is that the tables will just accumulate over time,
so you&rsquo;ll rack up storage costs. Fortunately, BigQuery makes it easy to give an
expiration time to tables, so you can set them to be automatically deleted
after a week (or however long you want to keep them).</p>

<p>The core BigBingo job has 7 queries/tables instead of 3, but it is designed
with the same strategy of keeping all old tables, and this strategy has helped
tremendously and kept BigBingo&rsquo;s data consistent in the face of all sorts of
errors:</p>

<ul>
<li>Various transient errors (connection timeouts, internal BigQuery errors,
etc.) have caused the whole BigBingo job to occasionally fail, and in these
cases, it&rsquo;s <em>always</em> safe to just retry the job.</li>
<li>The log import process has sometimes failed and sometimes taken too long
to run, and in both situations, BigBingo automatically fails (and sends an
email reporting the failure) because the data it depends on isn&rsquo;t ready yet.</li>
<li>Whenever BigBingo fails, all future BigBingo jobs fail (rather than computing
incorrect data) until the data is caught up.</li>
<li>Sometimes two instances of the job end up running at the same time. Since the
intermediate data is all timestamped, this doesn&rsquo;t cause any problems.</li>
<li>One time, when retrying a failed job, I accidentally gave an incorrect UNIX
timestamp. The wrong hour was processed, but it didn&rsquo;t hurt data integrity at
all.</li>
<li>In one or two cases, bugs have made the data actually incorrect for a while.
Repairing the system is easy: just fix the bug and re-run the BigBingo job from
before the bug was introduced.</li>
</ul>


<p>The system is completely foolproof: I could replace cron with a thousand
monkeys repeatedly triggering BigBingo jobs with random UNIX timestamps, and
the system would still eventually make progress and remain completely
consistent (although it would be a little less cost-efficient). That level of
safety means I can stop worrying about maintenance and focus on more important
things.</p>

<h2>Where&rsquo;s the source code?</h2>

<p>Ideally, BigBingo would be a self-contained open-source library, but it
currently has enough dependencies on internal KA infrastructure that it&rsquo;s both
hard to make general and would be a bit difficult to use in isolation anyway.</p>

<p>That said, there&rsquo;s no reason I can&rsquo;t share the code, so
<a href="https://gist.github.com/alangpierce/f0ad63643b446a4f84ad">here&rsquo;s a Gist with pretty much all of the code</a>
(at the time of this blog post). I put an MIT license on it, so feel free to
base work off of it or use any of the self-contained pieces.</p>

<p>Khan Academy has
<a href="https://github.com/Khan">lots of open-source projects</a>, and it&rsquo;s not out of
the question for BigBingo to be made truly open source in the future, so let me
know in the comments if you think you would use it.</p>

<h2>That&rsquo;s all for now</h2>

<p>Curious about any more details? Think we&rsquo;re doing A/B testing all wrong?
Let me know in the comments!</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Alan Pierce</span></span>

      








  


<time datetime="2014-07-07T12:00:00-07:00" pubdate data-updated="true">Jul 7<sup>th</sup>, 2014</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://www.alangpierce.com/blog/2014/07/07/bigbingo-khan-academys-new-bigquery-backed-a-b-testing-framework/" data-via="" data-counturl="http://www.alangpierce.com/blog/2014/07/07/bigbingo-khan-academys-new-bigquery-backed-a-b-testing-framework/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/07/07/bigbingo-khan-academys-new-bigquery-backed-a-b-testing-framework/">BigBingo: Khan Academy's New BigQuery-backed A/B Testing Framework</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Alan Pierce -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'alangpierce';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://www.alangpierce.com/blog/2014/07/07/bigbingo-khan-academys-new-bigquery-backed-a-b-testing-framework/';
        var disqus_url = 'http://www.alangpierce.com/blog/2014/07/07/bigbingo-khan-academys-new-bigquery-backed-a-b-testing-framework/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
